"""Agent 8 – AI Peer Review Simulator: simulates a multi-reviewer conference review."""

from app.agents.base_agent import BaseAgent
from app.services.llm_service import generate_json


class PeerReviewAgent(BaseAgent):
    name = "peer_review"
    description = "Simulates a full academic peer review with multiple virtual reviewers"

    async def _execute(self, paper_text: str, context: dict) -> dict:
        extraction = context.get("structured_extractor", {})
        title = extraction.get("title", "")
        methodology = extraction.get("methodology", {})
        results = extraction.get("results", {})
        contributions = extraction.get("contributions", [])
        limitations = extraction.get("limitations", [])

        prompt = f"""You are simulating a full academic peer review process for a research paper submission.
Generate reviews from 3 different reviewers, each with a distinct expertise and perspective.

PAPER BEING REVIEWED:
Title: {title}
Text (excerpt): {paper_text[:10000]}
Methodology: {str(methodology)[:2000]}
Results: {str(results)[:2000]}
Contributions: {str(contributions)[:1000]}
Limitations: {str(limitations)[:1000]}

IMPORTANT: Create 3 genuinely different reviewers with contrasting viewpoints. One should be more positive, one more critical, and one balanced. Each should focus on different aspects based on their expertise.

Return a JSON object:
{{
    "conference": "AI/ML Conference 2026 (Simulated)",
    "paper_title": "{title}",
    "reviewers": [
        {{
            "reviewer_id": "R1",
            "expertise": "e.g. Deep Learning, NLP, Computer Vision",
            "confidence": 4,
            "overall_score": 7,
            "scores": {{
                "novelty": 7,
                "technical_quality": 6,
                "clarity": 8,
                "significance": 7,
                "reproducibility": 5,
                "experimental_design": 6
            }},
            "summary": "2-3 sentence summary of the paper from this reviewer's perspective",
            "strengths": ["strength 1", "strength 2", "strength 3"],
            "weaknesses": ["weakness 1", "weakness 2", "weakness 3"],
            "questions": ["question for authors 1", "question 2"],
            "detailed_comments": "A thorough 4-6 sentence review paragraph with specific feedback",
            "recommendation": "accept|weak_accept|borderline|weak_reject|reject"
        }}
    ],
    "meta_review": {{
        "decision": "Accept|Weak Accept|Borderline|Weak Reject|Reject",
        "average_score": 6.5,
        "consensus_summary": "2-3 sentences summarizing the reviewers' consensus",
        "key_strengths": ["Top 3 agreed-upon strengths"],
        "key_concerns": ["Top 3 agreed-upon concerns"],
        "recommendation_to_authors": "Specific actionable advice for improving the paper",
        "verdict_reasoning": "Why the meta-reviewer arrived at this decision"
    }},
    "review_quality_note": "This is a simulated peer review generated by AI. Actual peer reviews would involve domain experts reading the full paper."
}}

SCORING GUIDE:
- confidence: 1-5 (1=not expert, 5=expert in this exact area)
- overall_score: 1-10 (1=strong reject, 10=strong accept)
- Individual scores: 1-10
- Be realistic — most papers score 5-7, truly excellent ones 8-9

Generate 3 complete, distinct reviews. Be thorough and realistic.
"""
        return await generate_json(
            prompt,
            system_instruction="You are simulating the peer review process of a top academic conference. Generate realistic, detailed, and constructive reviews from 3 different expert perspectives. Be fair but rigorous.",
            agent_name=self.name,
        )
